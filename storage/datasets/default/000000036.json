{
  "url": "https://apify.com/apify/website-content-crawler",
  "title": "Website Content Crawler · Apify",
  "content": "![Website Content Crawler avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9hWUcwbDlzN2RiQjdqM2diUy9QZlRvRU5rSlp4YWh6UER1My1DbGVhblNob3RfMjAyMy0wMy0yOF9hdF8xMC40MC4yMF8yeC5wbmc.webp)](https://images.apifyusercontent.com/KU8LSu7typDKdVnsXyljAiKM8nhie0ogLCOv5-6Rqyk/rs:fill:250:250/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9hWUcwbDlzN2RiQjdqM2diUy9QZlRvRU5rSlp4YWh6UER1My1DbGVhblNob3RfMjAyMy0wMy0yOF9hdF8xMC40MC4yMF8yeC5wbmc.webp)\n![Website Content Crawler (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9hWUcwbDlzN2RiQjdqM2diUy9QZlRvRU5rSlp4YWh6UER1My1DbGVhblNob3RfMjAyMy0wMy0yOF9hdF8xMC40MC4yMF8yeC5wbmc.webp)](https://images.apifyusercontent.com/KU8LSu7typDKdVnsXyljAiKM8nhie0ogLCOv5-6Rqyk/rs:fill:250:250/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9hWUcwbDlzN2RiQjdqM2diUy9QZlRvRU5rSlp4YWh6UER1My1DbGVhblNob3RfMjAyMy0wMy0yOF9hdF8xMC40MC4yMF8yeC5wbmc.webp)\n![File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9ac2NNd0ZSNUg3ZUN0V3R5aC9ZcXRrUW1FeFpwbU1kNmRKUS1hcGlmeV9zeW1ib2xfd2hpdGVfYmcucG5n.webp](https://images.apifyusercontent.com/5jKI7lzsTnib7cZ9ZbovlHnNFfJa8QiIyuEPciDRilk/rs:fill:224:224/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9ac2NNd0ZSNUg3ZUN0V3R5aC9ZcXRrUW1FeFpwbU1kNmRKUS1hcGlmeV9zeW1ib2xfd2hpdGVfYmcucG5n.webp)\n![input-screenshot.png (File: aHR0cHM6Ly9hcGlmeS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9kOWYwZWI4Yi1hZTAxLTQyYmUtODYyOS0zOWU2NTlhMTRlZDZfd2Vic2l0ZV9jb250ZW50X2NyYXdsZXJfaW5wdXRfZXhhbXBsZS5wbmc.png)](https://images.apifyusercontent.com/jGeMBhI-267YyeBlkj1jsqyq2bIlE2Lb6KimTPWGShU/cb:1/aHR0cHM6Ly9hcGlmeS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9kOWYwZWI4Yi1hZTAxLTQyYmUtODYyOS0zOWU2NTlhMTRlZDZfd2Vic2l0ZV9jb250ZW50X2NyYXdsZXJfaW5wdXRfZXhhbXBsZS5wbmc.png)\n![page-screenshot.png (File: aHR0cHM6Ly9hcGlmeS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS8zOTlkZGVmZC0zODc3LTQxZTctODZlZC00MDI1Y2RlYTQ2ZjhfU2NyZWVuc2hvdDIwMjMtMDMtMjlhdDEwLjU2LjU3LnBuZw.png)](https://images.apifyusercontent.com/q16mXxWV11UVH5A6rMrtm7FAgwbyJOJJ2zTrz9W6mPo/cb:1/aHR0cHM6Ly9hcGlmeS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS8zOTlkZGVmZC0zODc3LTQxZTctODZlZC00MDI1Y2RlYTQ2ZjhfU2NyZWVuc2hvdDIwMjMtMDMtMjlhdDEwLjU2LjU3LnBuZw.png)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vUzVHOWdPTlZsZWQxeUhTRHNLNXNCS2x2elVTS0ptUzlEZUZqdnhQd25wby9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVGalNGUjBabTlQTFdOWVdtSndSbnB4T0VnMVRrZzNOMFZKY0RZNVh6VlJVR1pvUm5STFNUZDRYMUE.webp)](https://images.apifyusercontent.com/b79NtB1y7Xb5f3yNncLddsr4zoF1mGY63p0KG3YO4jA/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vUzVHOWdPTlZsZWQxeUhTRHNLNXNCS2x2elVTS0ptUzlEZUZqdnhQd25wby9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVGalNGUjBabTlQTFdOWVdtSndSbnB4T0VnMVRrZzNOMFZKY0RZNVh6VlJVR1pvUm5STFNUZDRYMUE.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vUzVHOWdPTlZsZWQxeUhTRHNLNXNCS2x2elVTS0ptUzlEZUZqdnhQd25wby9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVGalNGUjBabTlQTFdOWVdtSndSbnB4T0VnMVRrZzNOMFZKY0RZNVh6VlJVR1pvUm5STFNUZDRYMUE.webp)](https://images.apifyusercontent.com/b79NtB1y7Xb5f3yNncLddsr4zoF1mGY63p0KG3YO4jA/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vUzVHOWdPTlZsZWQxeUhTRHNLNXNCS2x2elVTS0ptUzlEZUZqdnhQd25wby9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVGalNGUjBabTlQTFdOWVdtSndSbnB4T0VnMVRrZzNOMFZKY0RZNVh6VlJVR1pvUm5STFNUZDRYMUE.webp)\n![AI Website Content Markdown Scraper avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vU3BDWWZWU0RRNzg1RXd5aGMvQmRobnN6T2dWYVZySExrejMtbWFya2Rvd25fY3Jhd2xlcl9sb2dvLnBuZw.webp)](https://images.apifyusercontent.com/z4ns7t3WR1dGkVh94heYhBnLpSyhSnnY9yYj7S21IRA/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vU3BDWWZWU0RRNzg1RXd5aGMvQmRobnN6T2dWYVZySExrejMtbWFya2Rvd25fY3Jhd2xlcl9sb2dvLnBuZw.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vdFcxYWV2SVVwb3VfaDE2N2RISzhUcHRsdENJUER1MHMyZzJ4VnNzbE5sQS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloZG1GMFlYSnpMbWRwZEdoMVluVnpaWEpqYjI1MFpXNTBMbU52YlM5MUx6YzNOVEk1T0RNMQ.webp)](https://images.apifyusercontent.com/VdTsuK_LjKJCcySWO1NE2d5-AVju4OMXzJpAMzE5mwk/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vdFcxYWV2SVVwb3VfaDE2N2RISzhUcHRsdENJUER1MHMyZzJ4VnNzbE5sQS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloZG1GMFlYSnpMbWRwZEdoMVluVnpaWEpqYjI1MFpXNTBMbU52YlM5MUx6YzNOVEk1T0RNMQ.webp)\n![Webpage Singer 🎶 avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vTGpBekVHMUNhZGxpRUNucm4tYWN0b3ItREtxNmhwd2JnV285Qmc3cFktWHpyV2J6SDRpRy1XZWJwYWdlX1Npbmdlci5wbmc.webp)](https://images.apifyusercontent.com/M9wdJ5oBJveqR79OrO4uSQOU-yxvr7-tIT_pmSqrwCE/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vTGpBekVHMUNhZGxpRUNucm4tYWN0b3ItREtxNmhwd2JnV285Qmc3cFktWHpyV2J6SDRpRy1XZWJwYWdlX1Npbmdlci5wbmc.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vQ3QtclVSMnlOMGtKdlZCcm9DajVCMVJfcEExUnJhUDFHSUUwQzBmZFBzMC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVObk9HOWpTalE1YlhwMmVGRkxSbTlRYWpKdWRsTnlOMHRvU0c0NVpIUnVObUpYVkVsUVlXOVhRalJKVDNWUlowWkZOM0Ju.webp)](https://images.apifyusercontent.com/PGgfwsjW7lQ8y_ycMctEhZid8OW5EYLrmFMoaWmQalQ/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vQ3QtclVSMnlOMGtKdlZCcm9DajVCMVJfcEExUnJhUDFHSUUwQzBmZFBzMC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVObk9HOWpTalE1YlhwMmVGRkxSbTlRYWpKdWRsTnlOMHRvU0c0NVpIUnVObUpYVkVsUVlXOVhRalJKVDNWUlowWkZOM0Ju.webp)\n![Actor Inspector Agent avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vYTZlbDZpZk1EZE83UlZWMzQtYWN0b3ItSFY5eEQ4QTRhc3RkUzQzTGItQmI4cFdQWk1YRi0zZWQ2YWQ5Mi04NzVkLTRjN2MtOWRmZi1hNjRhMzcyNzFjYzcucG5n.webp)](https://images.apifyusercontent.com/hOHH64Dep2bupts_c8nHBN47H0ehyN4E2TbqAUNqRMU/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vYTZlbDZpZk1EZE83UlZWMzQtYWN0b3ItSFY5eEQ4QTRhc3RkUzQzTGItQmI4cFdQWk1YRi0zZWQ2YWQ5Mi04NzVkLTRjN2MtOWRmZi1hNjRhMzcyNzFjYzcucG5n.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vaGh0eElrOHBRN3k5cEszVl9rZ050QUZKR0gzV3ozVEpuUVBMLXBCRFVhOC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11ZFhNdFpXRnpkQzB4TG1GdFlYcHZibUYzY3k1amIyMHZZVFpsYkRacFprMUVaRTgzVWxaV016UXZaWFJqUTA5b1NVODVSMlF3UWpOV1Vrb3RNVGN5TXpjMU1EY3lPRFU1T1M1cWNHVm4uanBlZw.webp)](https://images.apifyusercontent.com/H4YxyEoFGKkFf96gy75RstC7M_FvWCe1kNJDkPEC-Ng/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vaGh0eElrOHBRN3k5cEszVl9rZ050QUZKR0gzV3ozVEpuUVBMLXBCRFVhOC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11ZFhNdFpXRnpkQzB4TG1GdFlYcHZibUYzY3k1amIyMHZZVFpsYkRacFprMUVaRTgzVWxaV016UXZaWFJqUTA5b1NVODVSMlF3UWpOV1Vrb3RNVGN5TXpjMU1EY3lPRFU1T1M1cWNHVm4uanBlZw.webp)\n![Blog / Dated Content Crawler avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vT3BWT1c3cDkxV3V6ZEI4aE0tYWN0b3ItazZocG82WlZPd3ZoNXpiMU8ta2FFcEpBYktzdS1jZGE0NDBkMS1hOTlmLTRjOTEtYmJjZi04MzY5NzBjMDBkZGUuanBn.webp)](https://images.apifyusercontent.com/2wASJ--BQF28mBrPkl-VHTbDQFNO-qoifUX4vgxjcgw/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vT3BWT1c3cDkxV3V6ZEI4aE0tYWN0b3ItazZocG82WlZPd3ZoNXpiMU8ta2FFcEpBYktzdS1jZGE0NDBkMS1hOTlmLTRjOTEtYmJjZi04MzY5NzBjMDBkZGUuanBn.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vMlMtMWVRcDB5TWxwZXBPMWUxSHlma21LVmVfSnRnTUtJRlhWVDdnY0hZcy9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTkzZDNjdVozSmhkbUYwWVhJdVkyOXRMMkYyWVhSaGNpOWtObUk0TjJVNU1tVXpOV0ptT0RZeVlqVTROREptTjJJd01tWm1NamRrT1Q5a1BXaDBkSEJ6SlROQkpUSkdKVEpHWTJSdUxtRndhV1o1TG1OdmJTVXlSbWx0WnlVeVJtbGpiMjV6SlRKR1lXNXZibmx0YjNWelgzVnpaWEpmY0dsamRIVnlaUzV3Ym1j.webp)](https://images.apifyusercontent.com/ZsfzZb1pxTblDuLoIOY8G_bohjvIF6qQ6JKTx1jnBDg/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vMlMtMWVRcDB5TWxwZXBPMWUxSHlma21LVmVfSnRnTUtJRlhWVDdnY0hZcy9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTkzZDNjdVozSmhkbUYwWVhJdVkyOXRMMkYyWVhSaGNpOWtObUk0TjJVNU1tVXpOV0ptT0RZeVlqVTROREptTjJJd01tWm1NamRrT1Q5a1BXaDBkSEJ6SlROQkpUSkdKVEpHWTJSdUxtRndhV1o1TG1OdmJTVXlSbWx0WnlVeVJtbGpiMjV6SlRKR1lXNXZibmx0YjNWelgzVnpaWEpmY0dsamRIVnlaUzV3Ym1j.webp)\n![Backlink Opportunity Finder avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vdkVMYzlxVFRpNFBGcU50RlUtYWN0b3ItY1hyYzRCR1VEZFVudkhPMjItWGtLdnFrTEhnQS1IdWdlaWNvbnNMaW5rMDMucG5n.webp)](https://images.apifyusercontent.com/QMCiK3b9kMTgZW3jZzUUb-Y3W8NnR79bE5cGz5aj9RI/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vdkVMYzlxVFRpNFBGcU50RlUtYWN0b3ItY1hyYzRCR1VEZFVudkhPMjItWGtLdnFrTEhnQS1IdWdlaWNvbnNMaW5rMDMucG5n.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vV2JMcHh6U0dRV0xpLUlzd3Q0UUtZcEZDUWlJcTRHck5WeGpFczh5b0R4cy9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11ZFhNdFpXRnpkQzB4TG1GdFlYcHZibUYzY3k1amIyMHZka1ZNWXpseFZGUnBORkJHY1U1MFJsVXZPVk5vWjBoYVlWQTJiemRaU1hCa1YwVXRhV052YmpFeU9DNXdibWMucG5n.webp)](https://images.apifyusercontent.com/jRLSQe923-AZF5qiNGrNkBM5HUiZ1P4YoVdpVX9TgXU/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vV2JMcHh6U0dRV0xpLUlzd3Q0UUtZcEZDUWlJcTRHck5WeGpFczh5b0R4cy9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11ZFhNdFpXRnpkQzB4TG1GdFlYcHZibUYzY3k1amIyMHZka1ZNWXpseFZGUnBORkJHY1U1MFJsVXZPVk5vWjBoYVlWQTJiemRaU1hCa1YwVXRhV052YmpFeU9DNXdibWMucG5n.webp)\n![Example Website Screenshot Crawler avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vTDBRUENjZGFZczVkVHdJTHQvRFc2Y3BJZksxazVlblJJV1ItdW5uYW1lZC5wbmc.webp)](https://images.apifyusercontent.com/nhpCMsnVNEdtbLmEr9jwvFvlmYkVKgjSj-ppNfyGOvM/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vTDBRUENjZGFZczVkVHdJTHQvRFc2Y3BJZksxazVlblJJV1ItdW5uYW1lZC5wbmc.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vOVNlcWVwSVFpeUYzQ0VsR0RoRUtSdDdhVV8wajhENkdmcWxYelgta19WTS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVObk9HOWpTMUpHYnpaRlVWSmhjRTVqUWtOVU9YYzRka3BzZEdkQ1REVldjVlZyZDBnelpHWkhWVFpwUWtVelpFRklSbDlFYXc.webp)](https://images.apifyusercontent.com/FS64TPmA1gRcfixaaPZwt0Y1Nto8jrWununJ6OhIm4M/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vOVNlcWVwSVFpeUYzQ0VsR0RoRUtSdDdhVV8wajhENkdmcWxYelgta19WTS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTlzYURNdVoyOXZaMnhsZFhObGNtTnZiblJsYm5RdVkyOXRMMkV2UVVObk9HOWpTMUpHYnpaRlVWSmhjRTVqUWtOVU9YYzRka3BzZEdkQ1REVldjVlZyZDBnelpHWkhWVFpwUWtVelpFRklSbDlFYXc.webp)\n![Web Scraper avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9tb0pSTFJjODVBaXRBcnBOTi9abjh2YldUaWthN2FuQ1FNbi1TRC0wMi0wMi5wbmc.webp)](https://images.apifyusercontent.com/_K0OdNbttmY6qWcG0si-64NPkBN5Q1WyOZJiajbSKZQ/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMuYW1hem9uYXdzLmNvbS9tb0pSTFJjODVBaXRBcnBOTi9abjh2YldUaWthN2FuQ1FNbi1TRC0wMi0wMi5wbmc.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vM0MwcFdDRm5kb2hoZVZNSzhRbzJrUUxOR0k5WkZwZldKT0lPTnJkX0RVUS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11WVcxaGVtOXVZWGR6TG1OdmJTOWFjMk5OZDBaU05VZzNaVU4wVjNSNWFDOVpjWFJyVVcxRmVGcHdiVTFrTm1SS1VTMWhjR2xtZVY5emVXMWliMnhmZDJocGRHVmZZbWN1Y0c1bi5wbmc.webp)](https://images.apifyusercontent.com/-sRbQTC2v_n7vZm5ou80N_lhm_QN1rbx5N4HNEXYGcs/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vM0MwcFdDRm5kb2hoZVZNSzhRbzJrUUxOR0k5WkZwZldKT0lPTnJkX0RVUS9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloY0dsbWVTMXBiV0ZuWlMxMWNHeHZZV1J6TFhCeWIyUXVjek11WVcxaGVtOXVZWGR6TG1OdmJTOWFjMk5OZDBaU05VZzNaVU4wVjNSNWFDOVpjWFJyVVcxRmVGcHdiVTFrTm1SS1VTMWhjR2xtZVY5emVXMWliMnhmZDJocGRHVmZZbWN1Y0c1bi5wbmc.webp)\n![Deep Email, Phone, & Social Media Scraper avatar (File: aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vT05sYnR0NVlDRHJzNjlHbVAtYWN0b3ItZGZrU3RJQWJPUUh1cUloYWEtTnhpRlg4OVZlVi1nb2RlZXBzdWJ0bGVmdWxsLmpwZw.webp)](https://images.apifyusercontent.com/VmculHYsnzBk3_cdqjps7plTSI9U2oYTs_AsfQN_D3Y/rs:fill:76:76/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vT05sYnR0NVlDRHJzNjlHbVAtYWN0b3ItZGZrU3RJQWJPUUh1cUloYWEtTnhpRlg4OVZlVi1nb2RlZXBzdWJ0bGVmdWxsLmpwZw.webp)\n![User avatar (File: aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vU2J1YV81X2didVRsQ2ZhVk0wS3pRTWI3M3gwZHItRm5Gd1I0NFQ4MEdUNC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloZG1GMFlYSnpMbWRwZEdoMVluVnpaWEpqYjI1MFpXNTBMbU52YlM5MUx6SXpOVEV3TlRZNFAzWTlOQQ.webp)](https://images.apifyusercontent.com/qtU51u5b9CUiU4V-riXZ_Cihtfm0-Mdc18YGtKHgL1o/rs:fill:32:32/cb:1/aHR0cHM6Ly9pbWFnZXMuYXBpZnl1c2VyY29udGVudC5jb20vU2J1YV81X2didVRsQ2ZhVk0wS3pRTWI3M3gwZHItRm5Gd1I0NFQ4MEdUNC9yczpmaWxsOjMyOjMyL2NiOjEvYUhSMGNITTZMeTloZG1GMFlYSnpMbWRwZEdoMVluVnpaWEpqYjI1MFpXNTBMbU52YlM5MUx6SXpOVEV3TlRZNFAzWTlOQQ.webp)\n![Blog article image (File: aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy8yMDI0LzA3L3doYXRfaXNfcmV0cmlldmFsLWF1Z21lbnRlZF9nZW5lcmF0aW9uLnBuZw.webp)](https://images.apifyusercontent.com/s8R9-yMITzAPaR9f50v4AmMJoGjnn__pgHi7pKiFVCU/rs:fill:630:354/cb:1/aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy8yMDI0LzA3L3doYXRfaXNfcmV0cmlldmFsLWF1Z21lbnRlZF9nZW5lcmF0aW9uLnBuZw.webp)\n![Blog article image (File: aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy9zaXplL3cxMjAwLzIwMjMvMTIvTGxhbWFJbmRleC12cy1MYW5nQ2hhaW4uanBn.webp)](https://images.apifyusercontent.com/yaB8HKFs6xA54Fhk_d7D15mWt5nqKZWJfHRIYzuchis/rs:fill:630:354/cb:1/aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy9zaXplL3cxMjAwLzIwMjMvMTIvTGxhbWFJbmRleC12cy1MYW5nQ2hhaW4uanBn.webp)\n![Blog article image (File: aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy9zaXplL3cxMjAwLzIwMjMvMDYvVmVjdG9yLWRhdGFiYXNlcy5qcGc.webp)](https://images.apifyusercontent.com/nOvMRL2daSdac5Ni1BLI1-KC0D5oVgzuiwC2CO1dOFA/rs:fill:630:354/cb:1/aHR0cHM6Ly9ibG9nLmFwaWZ5LmNvbS9jb250ZW50L2ltYWdlcy9zaXplL3cxMjAwLzIwMjMvMDYvVmVjdG9yLWRhdGFiYXNlcy5qcGc.webp)\n![Apify logo (File: favicon.svg)](https://apify.com/img/favicon.svg)\n![GDPR image (File: GDPR_a807ac9a_ba0683f740.svg)](https://cdn-cms.apify.com/GDPR_a807ac9a_ba0683f740.svg)\n![SOC2 image (File: SOC_dc6cf5ae_f7999d700f.svg)](https://cdn-cms.apify.com/SOC_dc6cf5ae_f7999d700f.svg)\n![GetApp Apify user reviews image (File: Get_App_1f4079a540.png)](https://cdn-cms.apify.com/Get_App_1f4079a540.png)\n![Software Advice Apify reviews image (File: Softwareadvice_88026ee6c2.png)](https://cdn-cms.apify.com/Softwareadvice_88026ee6c2.png)\n![Capterra Apify user reviews image (File: Capterra_ce83db3070.png)](https://cdn-cms.apify.com/Capterra_ce83db3070.png)\n![G2 Apify user reviews image (File: G2_d80d60f2f8.png)](https://cdn-cms.apify.com/G2_d80d60f2f8.png)\n![TrustRadius Apify user reviews image (File: Trust_Radius_7ca0e0bdad.png)](https://cdn-cms.apify.com/Trust_Radius_7ca0e0bdad.png)\n![Crozdesk Apify user reviews image (File: Crozdesk_1d12ceb040.png)](https://cdn-cms.apify.com/Crozdesk_1d12ceb040.png)\n\nSkip to content\n\n[](/)\n\n[Get started](https://console.apify.com/sign-up)[Log in](https://console.apify.com/sign-in)\n\n  * Product\n\nBack\n\n[Start here!Get data with ready-made web scrapers for popular websitesBrowse 4,000+ Actors](/store)\n\nThe Apify platform\n\n    * [Apify StorePre-built web scraping tools](/store)\n    * [ActorsBuild and run serverless programs](/actors)\n    * [IntegrationsConnect with apps and services](/integrations)\n    * [StorageStore results for web scrapers](/storage)\n\nAnti-blocking\n\n    * [Anti-blockingScrape without getting blocked](/anti-blocking)\n    * [ProxyRotate scraper IP addresses](/proxy)\n\nOpen source\n\n    * [CrawleeWeb scraping and crawling library](https://crawlee.dev/)\n\n  * Solutions\n\nBack\n\nWeb data for\n\n    * [Enterprise](/enterprise)\n    * [Startups](/resources/startups)\n    * [Universities](/resources/universities)\n    * [Nonprofits](/resources/nonprofits)\n\nUse cases\n\n    * [Data for generative AI](/data-for-generative-ai)\n    * [Lead generation](/use-cases/lead-generation)\n    * [Market research](/use-cases/market-research)\n    * [Sentiment analysis](/use-cases/sentiment-analysis)\n    * [View more →](/use-cases)\n\nConsulting\n\n    * [Apify Professional Services](/professional-services)\n    * [Apify Partners](/partners)\n\n  * Developers\n\nBack\n\n    * [DocumentationFull reference for the Apify platform](https://docs.apify.com/)\n\nGet started\n\n    * [Web scraping academyCourses for beginners and experts](https://docs.apify.com/academy)\n    * [Code templatesPython, JavaScript, and TypeScript](/templates)\n    * [Deploy to ApifyWith CLI or GitHub integration](https://docs.apify.com/platform/actors/development/deployment)\n    * [Monetize your codePublish your scrapers and get paid](https://apify.com/partners/actor-developers)\n\nLearn\n\n    * [API reference](https://docs.apify.com/api)\n    * [CLI](https://docs.apify.com/cli/)\n    * [SDK](https://docs.apify.com/sdk)\n    * [Crawlee](https://crawlee.dev/)\n\n[Apify open source fair shareWe will support and reward every open-source project on Apify StoreJoin now](/partners/open-source-fair-share)\n\n  * Resources\n\nBack\n\n    * [Help and supportAdvice and answers about Apify](https://help.apify.com/en/)\n    * [Submit your ideasTell us the Actors you want](/ideas)\n    * [ChangelogSee what’s new on Apify](/change-log)\n    * [Customer storiesFind out how others use Apify](/success-stories)\n\nCompany\n\n    * [About Apify](/about)\n    * [Contact us](/contact)\n    * [Blog](https://blog.apify.com/)\n    * [Apify Partners](/partners)\n    * [Affiliate Program](/partners/affiliate)\n    * [JobsWe're hiring!](/jobs)\n\n[Join our DiscordTalk to scraping experts](https://discord.com/invite/jyEM2PRvMU)\n\n  * [Pricing](/pricing)\n\n  * [Contact sales](/contact-sales)\n\n\n\n\n[Contact sales](/contact-sales)[Log in](https://console.apify.com/sign-in)\n\n[Get started](https://console.apify.com/sign-up)\n\n##### Website Content Crawler\n\nPricing\n\nPay per usage\n\n[Try for free](https://console.apify.com/actors/aYG0l9s7dbB7j3gbS?addFromActorId=aYG0l9s7dbB7j3gbS)\n\n[Go to Store](/store/categories)\n\n# Website Content Crawler\n\n[Try for free](https://console.apify.com/actors/aYG0l9s7dbB7j3gbS?addFromActorId=aYG0l9s7dbB7j3gbS)\n\napify/website-content-crawler\n\nDeveloped by\n\n[Apify](/apify)\n\nMaintained by Apify\n\nCrawl websites and extract text content to feed AI models, LLM applications, vector databases, or RAG pipelines. The Actor supports rich formatting using Markdown, cleans the HTML, downloads files, and integrates well with 🦜🔗 LangChain, LlamaIndex, and the wider LLM ecosystem.\n\nPricing\n\nPay per usage\n\nMonthly users\n\n6.1k\n\nRuns succeeded\n\n>99%\n\nResponse time\n\n1.5 days\n\nLast modified\n\n6 days ago\n\n[AI](/store/categories/ai)[Developer tools](/store/categories/developer-tools)\n\n  * [README](/apify/website-content-crawler)\n  * [Input](/apify/website-content-crawler/input-schema)\n  * [API](/apify/website-content-crawler/api/python)\n  * [Issues](/apify/website-content-crawler/issues/open)\n  * [Changelog](/apify/website-content-crawler/changelog)\n\n\n\nWebsite Content Crawler is an [Apify Actor](https://docs.apify.com/platform/actors) that can perform a deep crawl of one or more websites and extract text content from the web pages. It is useful to download data from websites such as documentation, knowledge bases, help sites, or blogs.\n\nThe Actor was specifically designed to extract data for feeding, fine-tuning, or training large language models (LLMs) such as [GPT-4](https://openai.com/product/gpt-4), [ChatGPT](https://openai.com/blog/chatgpt), or [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/).\n\nWebsite Content Crawler has a simple input configuration so that it can be easily integrated into customer-facing products, where customers can enter just a URL of the website they want to have indexed by an AI application. You can retrieve the results using the API to formats such as JSON or CSV, which can be fed directly to your LLM, [vector database](https://blog.apify.com/what-is-a-vector-database/), or directly to ChatGPT.\n\n## Main features\n\nWebsite Content Crawler is built upon [Crawlee](https://crawlee.dev/), Apify's state-of-the-art open-source library for web scraping and crawling. The Actor can:\n\n  * Crawl JavaScript-enabled websites using **headless Firefox** or simple sites using **raw HTTP**.\n  * Circumvent **anti-scraping protections** using browser fingerprinting and proxies.\n  * Save web content in plain text, **Markdown** , or HTML.\n  * Crawl **pages behind a login**.\n  * **Download files** in PDF, DOC, DOCX, XLS, XLSX, or CSV formats.\n  * **Remove fluff** from pages like navigation, header, footers, ads, modals, or cookies warnings to improve the accuracy of the data.\n  * Load content of pages with **infinite scroll**.\n  * Use sitemaps to find more URLs on the website.\n  * **Scale gracefully** from tiny sites to sites with millions of pages by leveraging the Apify platform capabilities.\n  * Integrate with **🦜🔗LangChain** ,: **LlamaIndex** , **Haystack** , **Pinecone** , **Qdrant** , or **OpenAI Assistant**\n  * and much more...\n\n\n\nLearn about the key features and capabilities in the **Website Content Crawler Overview** video:\n\nStill unsure if the Website Content Crawler can handle your use case? Simply try it for free and see the results for yourself.\n\n## Designed for generative AI and LLMs\n\nThe results of Website Content Crawler can help you feed, fine-tune or train your large language models (LLMs) or provide context for prompts for ChatGPT. In return, the model will answer questions based on your or your customer's websites and content.\n\nTo learn more, check out our **Web Scraping Data for Generative AI** video on this topic, showcasing the Website Content Crawler:\n\n**Custom chatbots for customer support**\n\nCustomer service chatbots personalized on customer websites, such as documentation or knowledge bases, are one of the most promising use cases of AI and LLMs. Let your customers easily onboard by typing the URL of their site, and thus give your chatbot detailed knowledge of their product or service. Learn more about this use case in our [blog post](https://blog.apify.com/talk-to-your-website-with-large-language-models/).\n\n**Generate personalized content based on customer’s copy**\n\nChatGPT and LLMs can write articles for you, but they won’t sound like you wrote them. Feed all your old blogs into your model to make it sound like you. Alternatively, train the model on your customers’ blogs and have it write in their tone of voice. Or help their technical writers with making first drafts of new documentation pages.\n\n**Retrieval Augmented Generation (RAG) use cases**\n\nUse your website content to create an all-knowing AI assistant. The LLM-powered bot can then answer questions based on your website content, or even generate new content based on the existing one. This is a great way to provide a personalized experience to your customers or help your employees find the information they need faster.\n\n**Summarization, translation, proofreading at scale**\n\nGot some old docs or blogs that need to be improved? Use Website Content Crawler to scrape the content, feed it to the ChatGPT API, and ask it to summarize, proofread, translate, or change the style of the content.\n\n**Enhance your custom GPTs**\n\nUploading knowledge files gives custom OpenAI GPTs reliable information to refer to when generating answers. With Website Content Crawler, you can scrape data from any website to [provide your GPT with custom knowledge](https://blog.apify.com/custom-gpts-knowledge/).\n\n## How does it work?\n\nWebsite Content Crawler operates in three stages:\n\n  1. **Crawling** \\- Finds and downloads the right web pages.\n  2. **HTML processing** \\- Transforms the DOM of crawled pages to e.g. remove navigation, header, footer, cookie warnings, and other fluff.\n  3. **Output** \\- Converts the resulting DOM to plain text or Markdown and saves downloaded files.\n\n\n\nFor clarity, the input settings of the Actor are organized according to the above stages. Note that input settings have reasonable defaults—the only mandatory setting is the **Start URLs**.\n\n### Crawling\n\nWebsite Content Crawler only needs one or more **Start URLs** to run, typically the top-level URL of the documentation site, blog, or knowledge base that you want to scrape. The actor crawls the start URLs, finds links to other pages, and recursively crawls those pages, too, as long as their URL is under the start URL.\n\nFor example, if you enter the start URL `https://example.com/blog/`, the actor will crawl pages like `https://example.com/blog/article-1` or `https://example.com/blog/section/article-2`, but will skip pages like `https://example.com/docs/something-else`.\n\nYou can also force the crawler to skip certain URLs using the **Exclude URLs (globs)** input setting, which specifies an array of glob patterns matching URLs of pages to be skipped. Note that this setting affects only links found on pages, but not **Start URLs** , which are always crawled. For example, `https://{store,docs}.example.com/**` will exclude all URLs starting with `https://store.example.com/` and `https://docs.example.com/`. Or `https://example.com/**/*\\?*foo=*` exclude all URLs that contain `foo` query parameter with any value. You can learn more about globs and test them [here](https://www.digitalocean.com/community/tools/glob?comments=true&glob=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F%2A%2A&matches=false&tests=https%3A%2F%2Fexample.com%2Ftools%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F&tests=https%3A%2F%2Fexample.com%2Fdont_scrape_this%2F123%3Ftest%3Dabc&tests=https%3A%2F%2Fexample.com%2Fscrape_this).\n\nThe Actor automatically skips duplicate pages identified by the same [canonical URL](https://en.wikipedia.org/wiki/Canonical_link_element); those pages are loaded and counted towards the _Max pages_ limit but not saved to the results.\n\nWebsite Content Crawler provides various input settings to customize the crawling. For example, you can select the crawler type:\n\n  * **Adaptive switching between browser and raw HTTP client** \\- The crawler automatically switches between raw HTTP for static pages and Firefox browser (via Playwright) for dynamic pages, to get the maximum performance wherever possible.\n  * **Headless web browser** \\- Useful for modern websites with anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it requires more computing resources and is slower.\n  * **Stealthy web browser** (default) - Another headless web browser, but with anti-blocking measures enabled. Try this if you encounter bot protection while scraping. For best performance, use it with Apify Proxy.\n  * **Raw HTTP client** \\- High-performance crawling mode that uses raw HTTP requests to fetch the pages. It is faster and cheaper, but it might not work on all websites.\n  * **Raw HTTP client with JS execution (JSDOM)** [experimental] - A compromise between a browser and raw HTTP crawlers. Good performance and should work on almost all websites, including those with dynamic content. However, it is still experimental and might sometimes crash, so we don't recommend it in production settings yet.\n\n\n\nYou can also set additional input parameters such as a maximum number of pages, maximum crawling depth, maximum concurrency, proxy configuration, timeout, etc., to control the behavior and performance of the Actor.\n\n### HTML processing\n\nThe goal of the HTML processing step is to ensure each web page has the right content — neither less nor more.\n\nIf you're using a headless browser **Crawler type** , whenever a web page is loaded, the Actor can wait a certain time or scroll to a certain height to ensure all dynamic page content is loaded, using the **Wait for dynamic content** or **Maximum scroll height** input settings, respectively. If **Expand clickable elements** is enabled, the Actor tries to click various DOM elements to ensure their content is expanded and visible in the resulting text.\n\nOnce the web page is ready, the Actor transforms its DOM to remove irrelevant content in order to help you ensure you're feeding your AI models with relevant data to keep them accurate.\n\nFirst, the Actor removes DOM nodes matching the **Remove HTML elements (CSS selector)**. The provided default value attempts to remove all common types of modals, navigation, headers, or footers, as well as scripts and inline images to reduce the output HTML size.\n\nIf the **Extract HTML elements (CSS selector)** option is specified, the Actor only keeps the contents of the elements targeted by this CSS selector and removes all the other HTML elements from the DOM.\n\nThen, if **Remove cookie warnings** is enabled, the Actor removes cookie warnings using the [I don't care about cookies](https://addons.mozilla.org/en-US/firefox/addon/i-dont-care-about-cookies/) browser extension.\n\nFinally, the Actor transforms the page using the selected **HTML transformer** , whose goal is to only keep the important content of the page and reduce its complexity before converting it to text. Basically, to keep just the \"meat\" of the article or a page.\n\n### File download\n\nIf the `Save files` option is set, the Actor will download \"document\" files linked from the page. This is limited to PDF, DOC, DOCX, XLS, and XLSX files.\n\nNote that these files are exempt from the URL scoping rules - any file linked from the page will be downloaded, regardless of its URL. You can change this behaviour by using the `Include / Exclude URLs (globs)` setting.\n\nThe hard limit for a single file download time is 1 hour. If the download of a single file takes longer, the Actor will abort the download.\n\nFurthermore, you can specify the minimal file download speed in kilobytes per second. If the download speed stays below this threshold for more than 10 seconds, the download will be aborted. This is configurable via the `Minimal file download speed (KB/s)` setting.\n\n### Output\n\nOnce the web page HTML is processed, the Actor converts it to the desired output format, including plain text, Markdown to preserve rich formatting, or save the full HTML or a screenshot of the page, which is useful for debugging. The Actor also saves important metadata about the content, such as author, language, publishing date, etc.\n\nThe results of the actor are stored in the default [Dataset](https://docs.apify.com/platform/storage/dataset) associated with the Actor run, from where you can access it via API and export to formats like JSON, XML, or CSV.\n\n## Example\n\nThis example shows how to scrape all pages from the Apify documentation at <https://docs.apify.com/>:\n\n### Input\n\n[See full input](https://apify.com/apify/website-content-crawler/input-schema) with description.\n\n### Output\n\nThis is how one crawled page (<https://docs.apify.com/academy/web-scraping-for-beginners>) looks in a browser:\n\nAnd here is how the crawling result looks in JSON format (note that other formats like CSV or Excel are also supported). The main page content can be found in the `text` field, and it only contains the valuable content, without menus and other noise:\n    \n    \n    1{\n    2    \"url\": \"https://docs.apify.com/academy/web-scraping-for-beginners\",\n    3    \"crawl\": {\n    4        \"loadedUrl\": \"https://docs.apify.com/academy/web-scraping-for-beginners\",\n    5        \"loadedTime\": \"2023-04-05T16:26:51.030Z\",\n    6        \"referrerUrl\": \"https://docs.apify.com/academy\",\n    7        \"depth\": 0\n    8    },\n    9    \"metadata\": {\n    10        \"canonicalUrl\": \"https://docs.apify.com/academy/web-scraping-for-beginners\",\n    11        \"title\": \"Web scraping for beginners | Apify Documentation\",\n    12        \"description\": \"Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\",\n    13        \"author\": null,\n    14        \"keywords\": null,\n    15        \"languageCode\": \"en\"\n    16    },\n    17    \"screenshotUrl\": null,\n    18    \"text\": \"Skip to main content\\nOn this page\\nWeb scraping for beginners\\nLearn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\\nWelcome to Web scraping for beginners, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you're looking for a quick start, we recommend trying this tutorial instead.\\nThis course is made by Apify, the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you'll be able to run your scrapers on any computer. No Apify account needed.\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the Apify platform course, where we'll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\nWhy learn scraper development?​\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\nCourse Summary​\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\nThis is what you'll learn in the Web scraping for beginners course:\\nWeb scraping for beginners\\nBasics of data extraction\\nBasics of crawling\\nBest practices\\nRequirements​\\nYou don't need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don't be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you're new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using [] instead of () can make a lot of difference.\\nIf you don't already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a JavaScript course and learning about CSS Selectors.\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\nIdeally, you should have at least a moderate understanding of the following concepts:\\nJavaScript + Node.js​\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and async...await), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\nasync...await (YouTube)\\nJavaScript loops (MDN)\\nModularity in Node.js\\nGeneral web development​\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be assumed (unless we're showing something out of the ordinary).\\nHTML\\nHTTP protocol\\nDevTools\\njQuery or Cheerio​\\nWe'll be using the Cheerio package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\nNext up​\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So let's get to it!\\nIf you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the Basics of crawling section.\\nWhy learn scraper development?\\nCourse Summary\\nRequirements\\nJavaScript + Node.js\\nGeneral web development\\njQuery or Cheerio\\nNext up\",\n    19    \"html\": null,\n    20    \"markdown\": \"  Web scraping for beginners | Apify Documentation       \\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)\\n\\nOn this page\\n\\n# Web scraping for beginners\\n\\n**Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.**\\n\\n* * *\\n\\nWelcome to **Web scraping for beginners**, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you're looking for a quick start, we recommend trying [this tutorial](https://blog.apify.com/web-scraping-javascript-nodejs/) instead.\\n\\nThis course is made by [Apify](https://apify.com), the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you'll be able to run your scrapers on any computer. No Apify account needed.\\n\\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the [Apify platform course](/academy/apify-platform), where we'll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\\n\\n## Why learn scraper development?[​](#why-learn \\\"Direct link to Why learn scraper development?\\\")\\n\\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\\n\\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\\n\\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technology like React or Next.js will be a piece of cake.\\n\\n## Course Summary[​](#summary \\\"Direct link to Course Summary\\\")\\n\\nWhen we set out to create the Academy, we wanted to build a complete guide to modern web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\\n\\nThis is what you'll learn in the **Web scraping for beginners** course:\\n\\n*   [Web scraping for beginners](/academy/web-scraping-for-beginners)\\n    *   [Basics of data extraction](/academy/web-scraping-for-beginners/data-collection)\\n    *   [Basics of crawling](/academy/web-scraping-for-beginners/crawling)\\n    *   [Best practices](/academy/web-scraping-for-beginners/best-practices)\\n\\n## Requirements[​](#requirements \\\"Direct link to Requirements\\\")\\n\\nYou don't need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don't be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you're new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using `[]` instead of `()` can make a lot of difference.\\n\\n> If you don't already have basic programming knowledge and would like to be well-prepared for this course, we recommend taking a [JavaScript course](https://www.codecademy.com/learn/introduction-to-javascript) and learning about [CSS Selectors](https://www.w3schools.com/css/css_selectors.asp).\\n\\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\\n\\nIdeally, you should have at least a moderate understanding of the following concepts:\\n\\n### JavaScript + Node.js[​](#javascript-and-node \\\"Direct link to JavaScript + Node.js\\\")\\n\\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and `async...await`), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\\n\\n*   [`async...await` (YouTube)](https://www.youtube.com/watch?v=vn3tm0quoqE&ab_channel=Fireship)\\n*   [JavaScript loops (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration)\\n*   [Modularity in Node.js](https://www.section.io/engineering-education/how-to-use-modular-patterns-in-nodejs/)\\n\\n### General web development[​](#general-web-development \\\"Direct link to General web development\\\")\\n\\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because the knowledge of them will be **assumed** (unless we're showing something out of the ordinary).\\n\\n*   [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)\\n*   [HTTP protocol](https://developer.mozilla.org/en-US/docs/Web/HTTP)\\n*   [DevTools](/academy/web-scraping-for-beginners/data-collection/browser-devtools)\\n\\n### jQuery or Cheerio[​](#jquery-or-cheerio \\\"Direct link to jQuery or Cheerio\\\")\\n\\nWe'll be using the [**Cheerio**](https://www.npmjs.com/package/cheerio) package a lot to parse data from HTML. This package provides a simple API using jQuery syntax to help traverse downloaded HTML within Node.js.\\n\\n## Next up[​](#next \\\"Direct link to Next up\\\")\\n\\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. So [let's get to it!](/academy/web-scraping-for-beginners/introduction)\\n\\n> If you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the [Basics of crawling](/academy/web-scraping-for-beginners/crawling) section.\\n\\n*   [Why learn scraper development?](#why-learn)\\n*   [Course Summary](#summary)\\n*   [Requirements](#requirements)\\n    *   [JavaScript + Node.js](#javascript-and-node)\\n    *   [General web development](#general-web-development)\\n    *   [jQuery or Cheerio](#jquery-or-cheerio)\\n*   [Next up](#next)\"\n    21}\n\n## Integration with the AI ecosystem\n\nThanks to the native [Apify platform integrations](https://docs.apify.com/platform/integrations), Website Content Crawler can seamlessly connect with various third-party systems and tools.\n\n### Exporting GPT knowledge files\n\nApify allows you to seamlessly export the results of Website Content Crawler runs to your custom GPTs.\n\nTo do this, go to the Output tab of the Actor run and click the \"Export results\" button. From here, pick `JSON` and click \"Export\". You can then upload the JSON file to your custom GPTs.\n\nFor a step-by-step guide, see [How to add a knowledge base to your GPTs](https://blog.apify.com/custom-gpts-knowledge/#how-to-add-knowledge-to-gpts-step-by-step-guide).\n\n### LangChain integration\n\n[LangChain](https://github.com/hwchase17/langchain) is the most popular framework for developing applications powered by language models. It provides an [integration for Apify](https://python.langchain.com/en/latest/modules/agents/tools/examples/apify.html), so you can feed Actor results directly to LangChain’s vector databases, enabling you to easily create ChatGPT-like query interfaces to websites with documentation, knowledge base, blog, etc.\n\n#### Python example\n\nFirst, install LangChain with OpenAI LLM and Apify API client for Python:\n    \n    \n    pip install apify-client langchain langchain_community langchain_openai openai tiktoken\n\nAnd then create a ChatGPT-powered answering machine:\n    \n    \n    1import os\n    2\n    3from langchain.indexes import VectorstoreIndexCreator\n    4from langchain_community.utilities import ApifyWrapper\n    5from langchain_core.document_loaders.base import Document\n    6from langchain_openai import OpenAI\n    7from langchain_openai.embeddings import OpenAIEmbeddings\n    8\n    9# Set up your Apify API token and OpenAI API key\n    10os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\n    11os.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n    12\n    13apify = ApifyWrapper()\n    14\n    15# Run the Website Content Crawler on a website, wait for it to finish, and save its results into a LangChain document loader:\n    16loader = apify.call_actor(\n    17    actor_id=\"apify/website-content-crawler\",\n    18    run_input={\"startUrls\": [{\"url\": \"https://docs.apify.com/\"}], \"maxCrawlPages\": 10},\n    19    dataset_mapping_function=lambda item: Document(\n    20        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n    21    ),\n    22)\n    23# Initialize the vector database with the text documents:\n    24index = VectorstoreIndexCreator(embedding=OpenAIEmbeddings()).from_loaders([loader])\n    25\n    26# Finally, query the vector database:\n    27query = \"What is Apify?\"\n    28result = index.query_with_sources(query, llm=OpenAI())\n    29\n    30print(\"answer:\", result[\"answer\"])\n    31print(\"source:\", result[\"sources\"])\n\nThe query produces an answer like this:\n\n> _Apify is a platform for developing, running, and sharing serverless cloud programs. It enables users to create web scraping and automation tools and publish them on the Apify platform._\n> \n> <https://docs.apify.com/platform/actors>, <https://docs.apify.com/platform/actors/running/actors-in-store>, <https://docs.apify.com/platform/security>, <https://docs.apify.com/platform/actors/examples>\n\nFor details and Jupyter notebook, see [Apify integration for LangChain](https://python.langchain.com/docs/integrations/tools/apify/).\n\n#### Node.js example\n\nSee [detailed example](https://js.langchain.com/docs/modules/indexes/document_loaders/examples/web_loaders/apify_dataset) in LangChain for JavaScript.\n\n### LlamaIndex integration\n\n[LlamaIndex](https://docs.llamaindex.ai/) is a Python library that provides a central interface to connect LLMs with external data. The [Apify integration](https://llamahub.ai/l/readers/llama-index-readers-apify?from=) makes it easy to feed LlamaIndex applications with data crawled from the web.\n\nInstall all required packages:\n    \n    \n    pip install apify-client llama-index-core llama-index-readers-apify\n    \n    \n    1from llama_index.core import Document\n    2from llama_index.readers.apify import ApifyActor\n    3\n    4reader = ApifyActor(\"<My Apify API token>\")\n    5\n    6documents = reader.load_data(\n    7    actor_id=\"apify/website-content-crawler\",\n    8    run_input={\n    9        \"startUrls\": [{\"url\": \"https://docs.llamaindex.ai/en/latest/\"}]\n    10    },\n    11    dataset_mapping_function=lambda item: Document(\n    12        text=item.get(\"text\"),\n    13        metadata={\n    14            \"url\": item.get(\"url\"),\n    15        },\n    16    ),\n    17)\n\n### Vector database integrations (Pinecone, Qdrant)\n\nWebsite Content Crawler can be easily integrated with vector databases to store the crawled data for semantic search. Using Apify's [Pinecone](https://apify.com/apify/pinecone-integration) or [Qdrant](https://apify.com/apify/qdrant-integration) integration Actors, you can upload the results of Website Content Crawler directly into a vector database. The integrations support incremental updates, updating only the data that has changed since the last crawl. This helps to reduce costly embedding computation and storage operations, making it suitable for regular updates of large websites. Just set up the Pinecone integration Actor with Website Content Crawler using this [step-by-step guide](https://docs.apify.com/platform/integrations/pinecone).\n\n### GPT integration\n\nYou can use Website Content Crawler to add knowledge to your GPTs. Crawl a website and upload the scraped dataset to your custom GPT. The video tutorial below demonstrates how it works.\n\nYou can also use the Website Content Crawler together with the OpenAI Assistant to update its knowledge base with web content using the [OpenAI VectorStore Integration](https://apify.com/jiri.spilka/openai-vector-store-integration).\n\n## How much does it cost?\n\nWebsite Content Crawler is free to use—you only pay for the Apify platform usage consumed by the Actor. The exact price depends on the crawler type and settings, website complexity, network speed, and random circumstances.\n\nThe main cost driver of Website Content Crawler is the compute power, which is measured in the Actor compute units (CU): 1 CU corresponds to an actor with 1 GB of memory running for 1 hour. With the baseline price of $0.25/CU, from our tests, the actor usage costs **approximately** :\n\n  * $0.5 - $5 per 1,000 web pages with a headless browser, depending on the website\n  * $0.2 per 1,000 web pages with raw HTTP crawler\n\n\n\nNote that Apify's free plan gives you $5 free credits every month and access to [Apify Proxy](https://apify.com/proxy), which is sufficient for testing and low-volume use cases.\n\n## Troubleshooting\n\n  * The Actor works best for crawling sites with multiple URLs. For **extracting text or Markdown from a single URL** , you might prefer to use [RAG Web Browser](https://apify.com/apify/rag-web-browser) in the Standby mode, which is much faster and more efficient.\n  * If the **extracted text doesn’t contain the expected page content** , try to select another _Crawler type_. Generally, a headless browser will extract more text as it loads dynamic page content and is less likely to be blocked.\n  * If the **extracted text has more than expected page content** (e.g. navigation or footer), try to select another _HTML transformer_ , or use the _Remove HTML elements_ setting to skip unwanted parts of the page.\n  * If the **crawler is too slow** , try increasing the Actor memory and/or the _Initial concurrency_ setting. Note that if you set the concurrency too high, the Actor will run out of memory and crash, or potentially overload the target site.\n  * If the target website is blocking the crawler, make sure to use the **Stealthy web browser (Firefox+Playwright)** crawler type and use residential proxies\n  * The crawler **automatically restarts on crash** , and continues where it left off. But if it crashes more than 3 times per minute, the system fails the Actor run.\n\n\n\n## Help & support\n\nWebsite Content Crawler is under active development. If you have any feedback or feature ideas, please [submit an issue](https://console.apify.com/actors/aYG0l9s7dbB7j3gbS/issues).\n\n## Is it legal?\n\nWeb scraping is generally legal if you scrape publicly available non-personal data. What you do with the data is another question. Documentation, help articles, or blogs are typically protected by copyright, so you can't republish the content without the owner's permission.\n\nLearn more about the legality of web scraping in this [blog post](https://blog.apify.com/is-web-scraping-legal/). If you're not sure, please seek professional legal advice.\n\n## Pricing\n\nPricing model\n\nPay per usage\n\nThis Actor is paid per platform usage. The Actor is free to use, and you only pay for the Apify platform usage.\n\n## You might also like\n\n[FWFast Website Content Crawler6sigmag/fast-website-content-crawlerA high-performance web scraper that rapidly extracts and analyzes content from multiple websites simultaneously. Perfect for competitive research, content aggregation, and website structure analysis.David Deng3805.0/5](/6sigmag/fast-website-content-crawler)\n\n[DWDeep Website Content Crawler6sigmag/deep-website-content-crawlerScrape Failed Killer! A high-performance web scraper that rapidly extracts and analyzes content from multiple websites simultaneously. Perfect for competitive research, content aggregation, and website structure analysis.David Deng183](/6sigmag/deep-website-content-crawler)\n\n### [AI Website Content Markdown Scraperquaking_pail/ai-website-content-markdown-scraperThis Apify Actor, \"Website Content Crawler with Markdown Extraction,\" is designed to perform a comprehensive crawl of specified websites, extract their text content, convert it into Markdown format, and store it in a structured dataset. The extracted content is suitable for feeding LLMs.AI_Builder409](/quaking_pail/ai-website-content-markdown-scraper)\n\n### [Webpage Singer 🎶josef.prochazka/webpage-singerEver wondered what a website would sound like as a song? This Actor takes any webpage, turns its content into lyrics, and transforms it into a track in your favorite genre. Just drop in a URL, pick a style, and let the AI do the rest.Josef Procházka155.0/5](/josef.prochazka/webpage-singer)\n\n### [Actor Inspector Agentjakub.kopecky/actor-inspector-agentAgent Actor Inspector 🕵️‍♂️: An Apify Actor that rates others on docs 📝, inputs 🔍, code 💻, functionality ⚙️, performance ⏱️, and uniqueness 🌟. Config with actorId array, run, and review results. Helps devs improve, ensures quality, and guides users.Jakub Kopecký2](/jakub.kopecky/actor-inspector-agent)\n\n### [Blog / Dated Content Crawlerdiarmuidr/blog-content-crawlerCrawl an entire blog / knowledge base or filter to just the new content. Supporting relevant AI queries by filtering pages by dateDiarmuid95.0/5](/diarmuidr/blog-content-crawler)\n\n### [Backlink Opportunity Findereasyapi/backlink-opportunity-finder🔍 Discover high-quality backlink opportunities to boost your domain authority and search rankings. Extract valuable data about potential websites for building authoritative backlinks, including domain metrics, relevance analysis, and estimated SEO impact.EasyApi1](/easyapi/backlink-opportunity-finder)\n\n### [Example Website Screenshot Crawlerdz_omar/example-website-screenshot-crawlerAutomated website screenshot crawler using Pyppeteer and Apify. This open-source actor captures screenshots from specified URLs, uploads them to the Apify Key-Value Store, and provides easy access to the results, making it ideal for monitoring website changes and archiving web content.Abdlhakim hefaia22](/dz_omar/example-website-screenshot-crawler)\n\n### [Web Scraperapify/web-scraperCrawls arbitrary websites using the Chrome browser and extracts structured data from web pages using a provided JavaScript function. The Actor supports both recursive crawling and lists of URLs, and automatically manages concurrency for maximum performance.Apify77.5k4.5/5](/apify/web-scraper)\n\n### [Deep Email, Phone, & Social Media Scraperpeterasorensen/snacciA powerful tool that extracts emails, phone numbers, and social media profiles from any website. It intelligently navigates, prioritizing pages likely to have contact info - even deep in the site. Perfect for lead generation, market research, competitive analysis, and building contact databases.peterasorensen1845.0/5](/peterasorensen/snacci)\n\n### Related articles\n\n[What is retrieval-augmented generation (and why use it for LLMs)?Read more](https://blog.apify.com/what-is-retrieval-augmented-generation/)[LlamaIndex vs. LangChainRead more](https://blog.apify.com/llamaindex-vs-langchain/)[6 open-source Pinecone alternatives for LLMsRead more](https://blog.apify.com/pinecone-alternatives/)\n\nProduct\n\n  * [Apify Store](/store)\n\n  * [Integrations](/integrations)\n\n  * [Proxy](/proxy)\n\n  * [Crawlee](https://crawlee.dev/)\n\n\n\n\nDevelopers\n\n  * [Documentation](https://docs.apify.com/)\n\n  * [Code templates](/templates)\n\n  * [Deploy to Apify](https://docs.apify.com/platform/actors/development/deployment)\n\n  * [API reference](https://docs.apify.com/api)\n\n  * [Get paid on Apify](/partners/actor-developers)\n\n\n\n\nConsulting\n\n  * [Professional Services](/professional-services)\n\n  * [Apify Partners](/partners)\n\n\n\n\nSupport\n\n  * [Help & Support](https://help.apify.com/en/)\n\n  * [Submit your ideas](/ideas)\n\n  * [Forum](https://discord.apify.com/)\n\n\n\n\nCompany\n\n  * [About Apify](/about)\n\n  * [Contact us](/contact)\n\n  * [Events](https://lu.ma/apify)\n\n  * [Blog](https://blog.apify.com/)\n\n  * [Become an affiliate](/partners/affiliate)\n\n  * [Customer stories](/success-stories)\n\n  * [Changelog](/change-log)\n\n  * [JobsWe're hiring!](/jobs)\n\n\n\n\n[](/)\n\nSocials\n\n  * [](https://www.linkedin.com/company/apifytech/)\n  * [](https://x.com/apify)\n  * [](https://github.com/apify)\n  * [](https://www.youtube.com/apify)\n  * [](https://discord.com/invite/jyEM2PRvMU)\n  * [](https://www.tiktok.com/@apifytech)\n\n\n\nSecurity\n\n  * [](https://docs.apify.com/legal/gdpr-information)\n  * [](https://trust.apify.com/)\n\n\n\nReviews\n\n  * [](https://www.getapp.com/business-intelligence-analytics-software/a/apify/)\n  * [](https://www.softwareadvice.com/data-extraction/apify-profile/)\n  * [](https://www.capterra.com/p/150854/Apify/)\n  * [](https://www.g2.com/products/apify/reviews)\n  * [](https://www.trustradius.com/products/apify/reviews)\n  * [](https://crozdesk.com/software/apify)\n\n\n\n[Loading status...](https://status.apify.com/)\n\n[Terms of Use](https://docs.apify.com/legal/general-terms-and-conditions)\n\n[Privacy Policy](https://docs.apify.com/legal/privacy-policy)\n\n[Cookie Policy](https://docs.apify.com/legal/cookie-policy)\n\nCookie settings\n\n© 2025 Apify\n"
}